{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Graphics Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import data handling libraries\n",
    "import datetime as dt\n",
    "\n",
    "def helloWorld():\n",
    "  print(\"Hello, World!\")\n",
    "\n",
    "def loadAndCleanData(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    data = data.fillna(0)\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "def computeProbability(feature, bin, data):\n",
    "    # Count the number of datapoints in the bin\n",
    "    count = 0.0\n",
    "\n",
    "    for i,datapoint in data.iterrows():\n",
    "        # See if the data is in the right bin\n",
    "        if datapoint[feature] >= bin[0] and datapoint[feature] < bin[1]:\n",
    "            count += 1\n",
    "\n",
    "    # Count the total number of datapoints\n",
    "    totalData = len(data)\n",
    "\n",
    "    # Divide the number of people in the bin by the total number of people\n",
    "    probability = count / totalData\n",
    "\n",
    "    # Return the result\n",
    "    return probability\n",
    "\n",
    "def computeConfidenceInterval(data):\n",
    "      # Confidence intervals\n",
    "      npArray = 1.0 * np.array(data)\n",
    "      stdErr = scipy.stats.sem(npArray)\n",
    "      n = len(data)\n",
    "      return stdErr * scipy.stats.t.ppf((1+.95)/2.0, n - 1)\n",
    "\n",
    "def getEffectSize(d1,d2):\n",
    "    m1 = d1.mean()\n",
    "    m2 = d2.mean()\n",
    "    s1 = d1.std()\n",
    "    s2 = d2.std()\n",
    "\n",
    "    return (m1 - m2) / math.sqrt((math.pow(s1, 3) + math.pow(s2, 3)) / 2.0)\n",
    "\n",
    "def runTTest(d1,d2):\n",
    "    return scipy.stats.ttest_ind(d1,d2)\n",
    "\n",
    "# pip install statsmodels\n",
    "# vars is a string with our independent and dependent variables\n",
    "# \" dvs ~ ivs\"\n",
    "def runANOVA(dataframe, vars):\n",
    "    model = ols(vars, data=dataframe).fit()\n",
    "    aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "    return aov_table\n",
    "\n",
    "# Plot a timeline of my data\n",
    "def plotTimeline(data, time_col, val_col):\n",
    "    sns.lineplot(data=data, x=time_col, y=val_col)\n",
    "    plt.show()\n",
    "\n",
    "# Plot a timeline of my data broken down by each category (cat_col)\n",
    "def plotMultipleTimelines(data, time_col, val_col, cat_col):\n",
    "    sns.lineplot(data=data, x=time_col, y=val_col, hue=cat_col)\n",
    "    plt.show()\n",
    "\n",
    "# Run a linear regression over the data. Models an equation\n",
    "# as y = mx + b and returns the list [m, b].\n",
    "def runTemporalLinearRegression(data, x, y):\n",
    "    # Format our data for sklean by reshaping from columns to np arrays\n",
    "    x_col = data[x].map(dt.datetime.toordinal).values.reshape(-1,1)\n",
    "    y_col = data[y].values.reshape(-1, 1)\n",
    "\n",
    "    # Run the regression using an sklearn regression object\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(x_col, y_col)\n",
    "\n",
    "    # Compute the R2 score and print it. Good scores are close to 1\n",
    "    y_hat = regr.predict(x_col)\n",
    "    fitScore = r2_score(y_col, y_hat)\n",
    "    print(\"Linear Regression Fit: \" + str(fitScore))\n",
    "\n",
    "    # Plot linear regression against data. This will let us visually judge whether\n",
    "    # or not our model is any good. With small data, a high R2 doesn't always mean\n",
    "    # a good model: we can use our intuition as well.\n",
    "    plt.scatter(data[x], y_col, color='lightblue')\n",
    "    plt.plot(data[x], y_hat, color='red', linewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "    # y = mx + b\n",
    "    # Return m and b\n",
    "    return [regr.coef_[0][0], regr.intercept_[0]]\n",
    "\n",
    "\n",
    "# Define a logistic function that we can use to model logistic data without\n",
    "# requiring classification.\n",
    "def logistic(x, x0, m, b):\n",
    "    y = 1.0 / (1.0 + np.exp(-m*(x - x0) + b))\n",
    "    return (y)\n",
    "\n",
    "# Define a logistic modeling regression. Use this regression for modeling the\n",
    "# data rather than a classification. Note that your y value must be between\n",
    "# 0 and 1 for this function to work correctly.\n",
    "def runTemporalLogisticRegression(data, x, y):\n",
    "    # Process the data\n",
    "    x_col = data[x].map(dt.datetime.toordinal)\n",
    "    y_col = data[y]\n",
    "\n",
    "    # Give the curve a crappy fit to start with\n",
    "    # In this case, we'll start with x0 as the median and define a straight\n",
    "    # line between 0 and 1. The curve_fit function will adjust the line\n",
    "    # to minimize the residuals.\n",
    "    p0 = [np.median(x_col), 1, min(y_col)]\n",
    "    params, pcov = curve_fit(logistic, x_col, y_col, p0)\n",
    "\n",
    "    # Show the fit with the actual data in blue and the model in red. Note that\n",
    "    # m = params[1] and b = params[2].\n",
    "    plt.scatter(data[x], y_col, color='lightblue')\n",
    "    plt.plot(data[x], logistic(x_col, params[0], params[1], params[2]), color='red', linewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "    # Compute the fit using R2\n",
    "    # Recall that the function is 1 - (sum of squares residuals / sum of squares total)\n",
    "    residuals = y_col - logistic(x_col, params[0], params[1], params[2])\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((y_col - np.mean(y_col))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    print(\"Logistic Regression Fit: \" + str(r_squared))\n",
    "\n",
    "    return params\n",
    "\n",
    "def runPCA(df): \n",
    "    # Standardize our features to a unit distribution\n",
    "    # Each feature gets mapped to mean = 0, stddev = 1\n",
    "    target_features = df.select_dtypes(include=\"number\")\n",
    "    x = StandardScaler().fit_transform(target_features)\n",
    "    \n",
    "    # Run PCA on the standardized data\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(x)\n",
    "    \n",
    "    # Merge data with original dataframe\n",
    "    newDf = pd.DataFrame(data=components, columns=[\"Component 1\", \"Component 2\"])\n",
    "    return pd.concat([newDf, df], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
